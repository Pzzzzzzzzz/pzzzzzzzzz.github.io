---
title: "（改良）不同卷积核大小，步长以及是否补零的影响基于MNIST"
date: 2023-04-13T13:46:02+08:00
draft: false
---

深度学习：第三个实验

目的：学习不同的卷积块进行模型构建，如：vgg_block, ResNet_block, 等。

数据：手写体数据；

实验内容：

1. 利用手写体数据的分类，设计一个卷积神经网络；

2. 自行设计卷积块（vgg_block, ResNet_block），对数据进行分类。

3. 讨论不同的深度可能出现的问题，及问题发生的原因。

```python

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

# 定义 VGG 模块
class VGGBlock(nn.Module):
    def __init__(self, in_channels, out_channels, batch_norm=True, dropout=False):
        super(VGGBlock, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),
            nn.ReLU(inplace=True),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),
            nn.ReLU(inplace=True),
        )
        self.dropout = nn.Dropout(p=0.5) if dropout else nn.Identity()

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = F.max_pool2d(x, kernel_size=2, stride=2)
        x = self.dropout(x)
        return x

# 定义 LeNetWithVGGBlocks 模型
class LeNetWithVGGBlocks(nn.Module):
    def __init__(self):
        super(LeNetWithVGGBlocks, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.vggblock1 = VGGBlock(128, 256)
        self.vggblock2 = VGGBlock(256, 512)
        self.fc1 = nn.Linear(512 * 2 * 2, 1024)
        self.fc2 = nn.Linear(1024, 10)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))), kernel_size=2)
        x = F.max_pool2d(F.relu(self.bn3(self.conv3(x))), kernel_size=2)
        x = self.vggblock1(x)
        x = self.vggblock2(x)
        x = x.view(-1, 512 * 2 * 2)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# 加载 MNIST 数据集并完成数据预处理
batch_size = 100
test_batch_size = 1000
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('data', train=True, download=True, transform=transforms.Compose([
            transforms.Resize((32, 32)),
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])),
    batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('data', train=False, transform=transforms.Compose([
            transforms.Resize((32, 32)),
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])),
    batch_size=test_batch_size, shuffle=True)

# 定义优化器和损失函数
lr = 0.01
momentum = 0.9
model = LeNetWithVGGBlocks()
optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-5) #（用了L2正则化和dropout）
criterion = nn.CrossEntropyLoss()

# 训练模型并测试性能
num_epochs = 3
log_interval = 100
for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))

    # 在测试集上测试模型准确率
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    accuracy = 100. * correct / len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset), accuracy))
```
模型深度过浅：如果模型的深度不够，它可能难以抓取到数据集的更复杂的特征，从而无法进行精准的分类。很多时候需要通过增加模型深度来提高模型的表达能力和性能。

模型深度过深：如果模型的深度过深，可能导致梯度消失/梯度爆炸问题，使得模型无法训练，或者出现过拟合的情况。此时可尝试使用梯度裁剪等方法来解决问题。

数据过拟合：如果数据集过小，或者模型设计过于复杂，都可能导致数据过拟合问题。在这种情况下，可以考虑引入正则化、加入噪音等方法来解决问题。

数据质量问题：如果数据集中存在噪声、缺失值等问题，也会影响模型的训练效果。为了解决这些问题，可以进行数据预处理、数据清洗等操作。

其他问题：除此之外，还有可能出现学习率设置不合理、梯度消失、网络结构设计不合理等问题。针对这些问题，可以对模型和训练过程进行调优，以提高模型的性能和准确度。