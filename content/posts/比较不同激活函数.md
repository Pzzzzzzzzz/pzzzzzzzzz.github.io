---
title: "测试各种激活函数"
date: 2023-03-31T13:46:02+08:00
draft: false
---

深度学习:第一个实验
利用MNIST数据集上进行激活函数的训练实验实验要求:
利用手写体数据的数据集，对不同的激活函数进行分析和比较 (SELU，ELU，Leaky-ReLU,ReLU等)
网络可以用最简单的前馈全连接网络(自行设)
要写出分析结果，并讨论可能的原因

```python

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

#接着，我们需要加载MNIST数据集，并进行数据预处理：

train_dataset = datasets.MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(root='data/', train=False, transform=transforms.ToTensor())

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

#接下来，我们可以定义一个简单的前馈全连接神经网络，并选择不同的激活函数进行比较：

class Net(nn.Module):
    def __init__(self, activation):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'leaky_relu':
            self.activation = nn.LeakyReLU()
        elif activation == 'elu':
            self.activation = nn.ELU()
        elif activation == 'selu':
            self.activation = nn.SELU()

    def forward(self, x):
        x = x.view(-1, 784)
        x = self.fc1(x)
        x = self.activation(x)
        x = self.fc2(x)
        return x
    
#然后，我们可以定义训练函数和测试函数：

def train(net, dataloader, criterion, optimizer, device):
    net.train()
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(dataloader, 0):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    return running_loss / len(dataloader)

def test(net, dataloader, criterion, device):
    net.eval()
    correct = 0
    total = 0
    running_loss = 0.0
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    acc = 100.0 * correct / total
    return running_loss / len(dataloader), acc

#最后，我们可以定义训练代码，并使用Matplotlib绘制训练曲线和测试曲线：

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
activations = ['relu', 'leaky_relu', 'elu', 'selu']
train_loss_list = []
test_loss_list = []
test_acc_list = []

for activation in activations:
    net = Net(activation).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(net.parameters(), lr=0.01)

    train_loss_epoch_list = []
    test_loss_epoch_list = []
    test_acc_epoch_list = []
    for epoch in range(10):
        train_loss = train(net, train_loader, criterion, optimizer, device)
        test_loss, test_acc = test(net, test_loader, criterion, device)
        train_loss_epoch_list.append(train_loss)
        test_loss_epoch_list.append(test_loss)
        test_acc_epoch_list.append(test_acc)
        print('{:d}, {:s}, Train Loss: {:.4f}, Test Loss: {:.4f}, Test Acc: {:.2f}'.format(
              epoch+1, activation.upper(), train_loss, test_loss, test_acc))
    train_loss_list.append(train_loss_epoch_list)
    test_loss_list.append(test_loss_epoch_list)
    test_acc_list.append(test_acc_epoch_list)

fig1, axs1 = plt.subplots(2, 2, figsize=(12, 12))
fig2, axs2 = plt.subplots(2, 2, figsize=(12, 12))

for i, (activation, train_loss_epoch_list, test_loss_epoch_list, test_acc_epoch_list) in \
    enumerate(zip(activations, train_loss_list, test_loss_list, test_acc_list)):
    row, col = int(i/2), int(i%2)
    axs1[row, col].plot(train_loss_epoch_list, label='Train Loss', linewidth=0.8)
    axs1[row, col].plot(test_loss_epoch_list, label='Test Loss', linewidth=0.8)
    axs1[row, col].set_title(activation.upper())
    axs1[row, col].legend()
    axs1[row, col].grid(linestyle='--', linewidth=0.5, alpha=0.6)

    axs2[row, col].plot(test_acc_epoch_list, label='Test Acc', linewidth=0.8)
    axs2[row, col].set_title(activation.upper())
    axs2[row, col].legend()
    axs2[row, col].grid(linestyle='--', linewidth=0.5, alpha=0.6)

for ax in axs1.flat:
    ax.set(xlabel='Epoch', ylabel='Loss')
for ax in axs2.flat:
    ax.set(xlabel='Epoch', ylabel='Accuracy')

plt.show()



**可以将Loss做成曲线按照不同集合进行对比**
```
![](https://pzzzzzzzzz.github.io/images/2fb3359c66e0fb8b2b57f630fcb5b1a.png)
![](https://pzzzzzzzzz.github.io/images/ff91d5e0d0510025bd6b27dff9ceb52.png)

根据图像我们可以稍微分析一下哈

relu激活函数：在模型学习初期，relu函数表现出了最快的收敛速度，并先于其他激活函数达到了最优的测试准确率。但在后期，测试准确率的上升速度明显放缓，并且测试损失的下降速度明显变慢，这可能是由于该激活函数在训练过程中可能会出现神经元失活的情况。

leaky_relu激活函数：在整个训练过程中，leaky_relu函数表现出了较快的收敛速度，并且测试准确率的波动幅度较小，表现出了良好的稳定性能。相比于 relu，leaky_relu 的性能更加优异，而且可以避免神经元失活的问题，造成更好的结果。

elu激活函数：相比于其他激活函数，elu可以在整个训练过程中都保持较高的测试准确率，并且测试损失下降的速度也相对较快，在模型稳定性和准确率上具有一定的优势。但同时由于 elu 的计算代价较高，在一些复杂的网络结构中，可能会出现计算时间过长的问题。

selu激活函数：相对于其他激活函数，selu需要满足较多的限制条件，并且在训练过程中需要仔细地调整参数，使得模型具有可行性较强。在这个例子中，selu的表现较差，可能是由于模型参数未得到足够的优化。但是，相比于其他激活函数，selu的表现也取决于所处的网络结构，一些复杂的网络结构也许可以通过 selu 取得更优的性能。